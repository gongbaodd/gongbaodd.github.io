{"posts":[{"id":"2020/01/13/Website Scraping with Python.md","slug":"2020/01/13/website-scraping-with-python","body":"\n# Website Scraping with Python\n\n## Parsing robots.txt\n\npage 28\n\n## Using Beautiful Soup\n\nç”¨æ¥è§£æ HTML\n\n- page 56 - ä½¿ç”¨\n- page 101 - åˆ©ç”¨ strainer åªè§£ææƒ³è¦çš„æ•°æ®\n\n## Exporting the Data\n\n- page 80 - CSV\n- page 87 - JSON\n- page 90 - SQLite\n- page 97 - MongoDB\n\n## Using Scrapy\n\n- page 111\n\n## Handling JavaScript\n\n- page 186 - Splash\n- page 196 - Selenium\n\n## Cloud\n\n- page 206 - [Scrapy Cloud](https://scrapinghub.com/scrapy-cloud)\n  - page 211 - [mlab](https://mlab.com/)\n- page 216 - [PythonAnywhere](https://www.pythonanywhere.com/)\n","collection":"blog","data":{"type":"post","category":"book"}},{"id":"2020/03/09/JS.md","slug":"2020/03/09/js","body":"\n# è¯»å®Œã€Œä½ ä¸çŸ¥é“çš„ JSã€ç¬¬ä¸€ç‰ˆ\n\n![book cover](./you-dont-know-about-JS.jpg)\n\nã€Œä½ ä¸çŸ¥é“çš„ JSã€æ˜¯[getify](https://github.com/getify)çš„ä¸€æœ¬å…³äºæ·±ç©¶ JavaScript çš„ä¹¦ï¼Œä¸­æ–‡ç‰ˆåˆ†ä¸ºä¸‰å†Œï¼Œå‰ä¸€é˜µå­å‘ç°ä½œè€…æ­£åœ¨æ›´æ–°ç¬¬äºŒç‰ˆï¼Œè¯»äº†å‡ é¡µçœŸçš„ä¸é”™ï¼Œäºæ˜¯æ‰¾æ¥ç¬¬ä¸€ç‰ˆè¯»äº†ä¸‹ã€‚\n\nç¬¬ä¸€ç‰ˆç¬¬ä¸€å†Œå°±æ˜¯æ·±ç©¶è¯æ³•ã€ä¸Šä¸‹æ–‡ï¼Œè¿™äº›ä¸œè¥¿å¤§éƒ¨åˆ†ä¹¦éƒ½æçš„æ¯”è¾ƒå°‘ï¼Œè¯´å®è¯ï¼Œè™½ç„¶æˆ‘å·²ç»äº†è§£äº†ï¼Œä½†æ˜¯æ€»æ˜¯å¿˜ã€‚è¿™æœ¬ä¹¦ç¬¬äºŒç‰ˆæ¯”ç¬¬ä¸€ç‰ˆå¥½å¤ªå¤šäº†ï¼Œæœ‰å…´è¶£çš„å¯ä»¥å»è¯»ä¸€ä¸‹ã€‚\n\nç¬¬äºŒå†Œï¼Œå¼‚æ­¥ï¼Œå…³äº promise å’Œç”Ÿæˆå™¨ï¼Œåº”è¯¥æ˜¯ç°æœ‰çš„å…³äºå¼‚æ­¥å†™çš„æœ€è¯¦ç»†çš„ä¹¦äº†ï¼Œä¸è¿‡æˆ‘æ›´æ„Ÿå…´è¶£ä»–èƒ½æ›´æ·±åº¦è®²è§£ä¸€ä¸‹ observableï¼Œè™½ç„¶è¿™ä¸ªè¿˜ä¸æ˜¯ JavaScript çš„æ ‡å‡†ã€‚\n\nç¬¬ä¸‰å†Œï¼ŒES6 ç›¸å…³ï¼Œå¾ˆæœ‰è¶£çš„æ˜¯ï¼ŒçœŸæœ¬ä¹¦å‰åŠéƒ¨åˆ†éƒ½æ˜¯å…³äºç¬¬ä¸€ç¬¬äºŒå†Œçš„ç®€è¿°ï¼Œæ‰€ä»¥å¦‚æœä½ åˆšå¥½ä¸Šä¸äº†ç½‘åªèƒ½ä¹°ä¹¦çš„è¯ï¼Œç›´æ¥çœ‹è¿™ä¸€æœ¬ä¹Ÿå·®ä¸å¤šã€‚\n\nåé¢æˆ‘å¯èƒ½ä¼šè¯»ä¸€ä¸‹ä¹¦ä¸­æ¨èçš„[Functiional Light JS](https://github.com/getify/Functional-Light-JS)ï¼Œä»¥åŠ 2ality å†™çš„[Deep JavaScript: Theory and techniques](https://exploringjs.com/deep-js/)ã€‚\n","collection":"blog","data":{"type":"post","category":"book","tag":["JavaScript"]}},{"id":"2020/03/21/Functional-Light-JS.md","slug":"2020/03/21/functional-light-js","body":"\næ¯”è¾ƒç®€å•çš„ä¸€æœ¬[å¼€æºä¹¦](https://github.com/getify/Functional-Light-JS)ï¼Œå¤§æ¦‚ç®—å‡½æ•°å¼ç¼–ç¨‹çš„å…¥é—¨ä¹¦+JavaScript éƒ¨åˆ†ä»‹ç»äº†ã€‚\n\næ¨èç« èŠ‚ï¼š\n\n- ç¬¬å››ç« ï¼Œä¸ºä»€ä¹ˆè¦å‡å°‘å‡½æ•°å‚æ•°ä»¥åŠæŸ¯é‡ŒåŒ–ï¼Œè¿™æ ·æœ‰åˆ©äºä½¿ç”¨ compose å‡½æ•°ç»„åˆæ­¥éª¤ã€‚\n- ç¬¬å…«ç« ï¼Œé€’å½’ï¼Œåˆ©ç”¨ es6 ä¸Šé¢çš„å°¾é€’å½’è°ƒç”¨æé«˜ä»£ç å¯è¯»æ€§ã€‚\n- é™„å½• Aï¼Œæé«˜æ€§èƒ½ï¼Œå‡å°‘è¿ç®—æŸè€—ã€‚\n","collection":"blog","data":{"type":"post","category":"book","tag":["JavaScript","functinal-programming"]}},{"id":"2020/04/13/threejs-fundamentals-threejs.md","slug":"2020/04/13/threejs-fundamentals-threejs","body":"\n# threejs fundamentals åå‰¯å…¶å®çš„ threejs å…¥é—¨ä¹¦\n\næœ€è¿‘å‡ å¤©çœŸæ˜¯å¿™ï¼Œé™¤äº†è¯»è¿™æœ¬ä¹¦ï¼Œæˆ‘çš„å¹³æ¿ï¼ˆé…·æ¯”é­”æ–¹ Mix Plusï¼‰çªç„¶è¿›ä¸å»ç³»ç»Ÿäº†ï¼ŒæŸ¥äº†ä¸€ä¸‹ï¼ŒåŸæ¥å±±å¯¨æœ¬ç¡¬ç›˜è´¨é‡ä¸å¥½ï¼Œè«åå…¶å¦™å…¨æ¸…äº†ï¼ˆæƒŠï¼ï¼‰ï¼Œå¥½åœ¨åšäº†å¤‡ä»½ï¼Œå½“ç„¶å› ä¸ºä¸æƒ³èŠ±é’±ï¼Œæ²¡å†ä¹°ä¸€ä¸ª SATA SSD ç¡¬ç›˜ï¼ŒèŠ±äº†ä¸€äº›æ—¶é—´é‡åšç³»ç»Ÿã€‚\n\nç¥¸ä¸å•è¡Œï¼Œä¸»åŠ›æœº Dell inspiron 13 é£æ‰‡åäº†ï¼Œè™½ç„¶å·²ç»æŠ¥ä¿®ï¼Œä½†è¿˜åœ¨èµ°æµç¨‹ï¼Œè¿™ä½¿å¾—æˆ‘ç°åœ¨åªèƒ½ä»¥ 1.8GHzï¼ˆå…³é—­é£æ‰‡ï¼‰ å·¥ä½œ ğŸ˜­ã€‚\n\næ­£å¥½ï¼Œåˆšåšå®Œç³»ç»Ÿï¼Œæ‰€ä»¥è¿™ç¯‡æ–‡ç« æˆ‘äº‰å–åœ¨å¹³æ¿ä¸Šå†™ï¼Œä»¥ä¿è¯ç³»ç»Ÿå®‰è£…æ­£ç¡®ã€‚\n\nå¦å¤–ï¼Œå› ä¸ºæŠ¢åˆ°äº†ç¦ç”°çš„æ¶ˆè´¹åˆ¸ï¼Œæ‰€ä»¥è¿™æœ¬ä¹¦çš„å®ç°ï¼Œäº‰å–åœ¨æœ¬å‘¨å®Œæˆã€‚\n","collection":"blog","data":{"type":"post","category":"book","tag":["threejs","JavaScript","webGL"]}},{"id":"2020/06/06/Python.md","slug":"2020/06/06/python","body":"\n# è¯»ã€ŒPython æ·±åº¦å­¦ä¹ ã€æ•°å­¦åŸºç¡€\n\n- ã€Œå­¦ä¹ ã€æ˜¯æŒ‡æ‰¾åˆ°ä¸€ç»„æ¨¡å‹å‚æ•°ï¼Œä½¿ç»™å®šçš„è®­ç»ƒæ•°æ®æ ·æœ¬å’Œå¯¹åº”ç›®æ ‡ä¸Šçš„æŸå¤±å‡½æ•°æœ€å°åŒ–ã€‚\n- å­¦ä¹ çš„è¿‡ç¨‹ï¼šéšæœºå–åŒ…å«æ•°æ®æ ·æœ¬åŠå…¶ç›®æ ‡å€¼çš„æ‰¹é‡ï¼Œå¹¶è®¡ç®—æ‰¹é‡æŸå¤±ç›¸å¯¹äºç½‘ç»œå‚æ•°çš„æ¢¯åº¦ï¼ˆæ¢¯åº¦å¯ä»¥ç†è§£ä¸ºå¯¹äºå¼ é‡è®¡ç®—çš„å€’æ•°ï¼‰ã€‚éšåå°†ç½‘ç»œå‚æ•°æ²¿ç€æ¢¯åº¦çš„åæ–¹å‘ç¨ç¨ç§»åŠ¨ï¼ˆç§»åŠ¨è·ç¦»ç”±å­¦ä¹ ç‡æŒ‡å®šï¼‰ã€‚\n- æ•´ä¸ªå­¦ä¹ è¿‡ç¨‹ä¹‹æ‰€ä»¥èƒ½å¤Ÿå®ç°ï¼Œæ˜¯å› ä¸ºç¥ç»ç½‘ç»œæ˜¯ä¸€ç³»åˆ—å¯å¾®åˆ†çš„å¼ é‡è¿ç®—ï¼Œå› æ­¤å¯ä»¥ä½¿ç”¨æ±‚å¯¼çš„é“¾å¼æ³•åˆ™æ¥å¾—åˆ°æ¢¯åº¦å‡½æ•°ï¼Œè¿™ä¸ªå‡½æ•°å°†å½“å‰å‚æ•°å’Œå½“å‰æ•°æ®æ‰¹é‡æ˜ å°„ä¸ºä¸€ä¸ªæ¢¯åº¦å€¼ã€‚\n- ã€ŒæŸå¤±ã€æ˜¯è®­ç»ƒè¿‡ç¨‹ä¸­éœ€è¦æœ€å°åŒ–çš„é‡ï¼Œå› æ­¤å®ƒèƒ½å¤Ÿè¡¡é‡å½“å‰ä»»åŠ¡æ˜¯å¦å·²ç»æˆåŠŸè§£å†³ã€‚\n- ã€Œä¼˜åŒ–å™¨ã€æ˜¯æŸå¤±æ¢¯åº¦æ›´æ–°å‚æ•°çš„å…·ä½“æ–¹å¼ï¼Œæ¯”å¦‚ RMSProp ä¼˜åŒ–å™¨ï¼Œå¸¦åŠ¨é‡çš„éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰ç­‰ã€‚\n","collection":"blog","data":{"type":"post","category":"book","tag":["tensorflow"],"series":{"slug":"learn-tensorflow","name":"å¼ é‡éº»è¾£çƒ«"}}},{"id":"2020/06/07/WSL2-Keras.md","slug":"2020/06/07/wsl2-keras","body":"\n# åœ¨ WSL2 å®‰è£… Keras åŠå…¶ä¾èµ–\n\n## å®‰è£… python\n\né¦–å…ˆå®‰è£… python3 å’Œ pip3ï¼Œå¹¶ä¸”ä½¿ç”¨ USTC æºã€‚\n\n```shell\nsudo apt install python3 python3-pip python3-dev\npip3 install -i https://mirrors.ustc.edu.cn/pypi/web/simple pip -U\npip3 config set global.index-url https://mirrors.ustc.edu.cn/pypi/web/simple\n```\n\n## å®‰è£… python ç§‘å­¦å¥—ä»¶\n\n1. å®‰è£… BLAS åº“ï¼ˆOpenBLASï¼‰ï¼Œç¡®ä¿å¯ä»¥åœ¨ CPU ä¸Šé¢åšå¼ é‡è¿ç®—ã€‚\n\n   ```shell\n   sudo apt install build-essential cmake git unzip pkg-config libopenblas-dev liblapack-dev\n   ```\n\n2. å®‰è£… Python ç§‘å­¦å¥—ä»¶ï¼šNumpyã€SciPy å’Œ Matplotlibã€‚è¿™æ˜¯åšç§‘å­¦è®¡ç®—å¿…é¡»çš„ã€‚\n\n   ```shell\n    sudo apt install python3-numpy python3-scipy python3-matplotlib python3-yaml\n   ```\n\n3. å®‰è£… HDF5ï¼Œæœ€åˆç”± NASA å¼€å‘ï¼Œç”¨äºä¿å­˜æ•°å€¼æ•°æ®çš„å¤§æ–‡ä»¶ã€‚å®ƒå¯ä»¥å¸®åŠ© Keras æ¨¡å‹å¿«é€Ÿé«˜æ•ˆä¿å­˜åˆ°ç£ç›˜ã€‚\n\n   ```shell\n    sudo apt install libhdf5-serial-dev python3-h5py\n   ```\n\n4. å®‰è£… Graphviz å’Œ pydot-ngï¼Œè¿™ä¸¤ä¸ªåŒ…ç”¨äºå¯è§†åŒ– Keras æ¨¡å‹ã€‚å¹¶ä¸æ˜¯å¿…é¡»çš„ã€‚\n\n   ```shell\n    sudo apt install graphviz\n    pip3 install pydot-ng\n   ```\n\n5. å®‰è£… tensorflow\n\n   ```shell\n   pip3 install tensorflow\n   pip3 install tensorflow-gpu\n   ```\n\n## å®‰è£… Keras\n\nä½¿ç”¨ pip3 å®‰è£… Kerasã€‚\n\n```shell\npip3 install keras\n```\n\nå¯ä»¥ä» GitHub ä¸­ä¸‹è½½ Keras çš„ä¾‹å­\n\n```shell\ngit clone https://github.com/keras-team/keras.git\n```\n\næµ‹è¯•ä¸€ä¸ªä¾‹å­\n\n```shell\npython3 mnist_cnn.py\n```\n\næ‰§è¡ŒæˆåŠŸåä¼šåœ¨`~/.keras/keras.json`ç”Ÿæˆé…ç½®æ–‡ä»¶ã€‚\n","collection":"blog","data":{"type":"post","category":"book","tag":["tensorflow","keras","wsl2","python"],"series":{"slug":"learn-tensorflow","name":"å¼ é‡éº»è¾£çƒ«"}}},{"id":"2020/06/08/Keras.md","slug":"2020/06/08/keras","body":"\n# Keras åˆ†ç±»é—®é¢˜-ç”µå½±è¯„è®ºåˆ†ç±»\n\n> ã€ŒPython æ·±åº¦å­¦ä¹ ã€çš„ä¾‹å­åœ¨[GitHub](https://github.com/fchollet/deep-learning-with-python-notebooks)ä¸Šã€‚\n> åˆ«é—®æˆ‘ä¸ºå•¥ä¸å†™åœ¨ notebook ä¸Šï¼Œç­‰æˆ‘å¿ƒæƒ…å¥½å°±ç»™ç½‘ç«™åŠ ä¸€ä¸‹è¿™ç»™ä½ åŠŸèƒ½å§\n\næœ¬æ–‡çš„ notebook åœ¨[è¿™ä¸ªè¿æ¥](https://github.com/gongbaodd/keras_study/blob/master/3.5%20movie%20reviews.ipynb)ã€‚\n\nå°† IMDB ä¸Šçš„ 50000 æ¡ä¸¤æåˆ†åŒ–çš„è¯„è®ºï¼Œä¸€èˆ¬ç”¨äºè®­ç»ƒï¼Œä¸€åŠç”¨äºæµ‹è¯•ã€‚è¿™äº›æ•°æ®ä¸­ data æ˜¯è¯ç´¢å¼•ç»„æˆçš„äºŒç»´æ•°ç»„ï¼Œæ¯ä¸€è¡Œå¯¹åº”æ¯æ¡è¯„è®ºï¼Œlabel ä¸º 0\\1 æ•°ç»„ï¼Œ0 è¡¨ç¤ºè´Ÿé¢è¯„è®ºï¼Œ1 è¡¨ç¤ºç§¯æè¯„è®ºã€‚\n\n## å¤„ç†æ•°æ®\n\n```py\nimport keras\nfrom keras.datasets import imdb\n\n(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = 1000)\n```\n\næ•°æ®é›†é‡Œé¢çš„æ•°å­—å¯ä»¥è§£ææˆè¯„è®ºï¼Œæ¯ä¸€ä¸ª`data`éƒ½æ˜¯å¦‚`[0, 14, 32, 56...]`çš„æ•°ç»„ã€‚\n\n```py\nword_index = imdb.get_word_index()\nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\ndecoded_review = ' '.join([reverse_word_index.get(i-3, '?') for i in train_data[0]])\n```\n\nä½¿ç”¨ä»¥ä¸‹æ–¹æ³•å³å¯ä»¥æŠŠè®­ç»ƒæ•°æ®å¤„ç†æˆ 0/1 äºŒç»´å¼ é‡ã€‚\n\n```py\nimport numpy as np\n\ndef vectorize_sequences(sequences, dimension=10000):\n    results = np.zeros((len(sequences), dimension))\n    for i, sequence in enumerate(sequences):\n        results[i, sequence] = 1\n    return results\n\nx_train = vectorize_sequences(train_data)\nx_test = vectorize_sequesnces(test_data)\n\ny_train = np.array(train_labels).astype('float32')\ny_test = np.array(test_labels).astype('float32')\n```\n\nç®€å•åœ°è¯´ï¼Œx å€¼ä¸º 25000\\*10000 çš„äºŒç»´æ•°ç»„ï¼ŒæŸä¸ªå•è¯å‡ºç°åˆ™åœ¨å®ƒæ‰€åœ¨çš„è¯„è®ºåºå·å’Œå¯¹åº”çš„å•è¯ç´¢å¼•çš„ä½ç½®ä¸Šèµ‹å€¼ 1ï¼Œè€Œ y å¯¹åº”æ¯ä¸ª x è¡Œè¯„è®ºçš„æ­£è´Ÿåé¦ˆã€‚\n\n## æ„å»ºç½‘ç»œ\n\nä½¿ç”¨ rlu æ¿€æ´»ï¼ˆçº¿æ€§æ•´æµå‡½æ•° `max(0, input)`ï¼‰çš„å…¨é“¾æ¥å±‚(Dense)å°±èƒ½å¾ˆå¥½å¤„ç†è¿™ç§è¾“å…¥å€¼ä¸ºæ ‡é‡å’Œå‘é‡çš„æƒ…å†µï¼Œå¦‚`Dense(16, activation='relu')`ã€‚å‰é¢çš„ 16 æ˜¯ä¸€ä¸ªéšè—å•å…ƒï¼Œè¡¨ç¤ºä¼šå°†æ•°æ®è¡¨ç¤ºåœ¨ä¸€ä¸ª 16 ç»´çš„è¡¨ç¤ºç©ºé—´ä¸­ï¼Œéšè—å•å…ƒè¶Šé«˜ï¼Œèƒ½å¤Ÿå­¦åˆ°çš„ç½‘ç»œè¶Šå¤æ‚ï¼Œè®¡ç®—ä»£ä»·ä¹Ÿè¶Šå¤§ï¼Œä½†å¹¶ä¸æ˜¯è¶Šé«˜è¶Šå¥½ï¼Œå¾€å¾€é«˜äº†ä¼šå­¦åˆ°ä¸æ­£ç¡®çš„æ¨¡å¼ã€‚\n\nä¸¤ä¸ªä¸­é—´ç½‘ç»œï¼Œç¬¬ä¸‰å±‚ä½¿ç”¨ sigmoid æ¿€æ´»ï¼ˆs å‡½æ•°ï¼‰ä»¥è¾“å‡º 0~1 çš„å€¼\n\n![Så‡½æ•°](https://wikimedia.org/api/rest_v1/media/math/render/svg/a26a3fa3cbb41a3abfe4c7ff88d47f0181489d13)\n\n```py\nfrom keras import models\nfrom keras import layers\nfrom keras import optimizers\nfrom keras import losses\nfrom keras import metrics\n\nmodel = models.Sequential()\nmodel.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\nmodel.add(layers.Dense(16, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer=optimizers.RMSprop(lr=0.001),\n              loss=losses.binary_crossentropy,\n              metrics=[metrics.binary_accuracy])\n```\n\nç¼–è¯‘ç½‘ç»œéœ€è¦é€‰æ‹©æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ï¼Œè¿™é‡Œæ˜¯ä¸ªäºŒåˆ†ç±»é—®é¢˜ï¼Œç½‘ç»œæœ€ç»ˆè¾“å‡ºä¸€ä¸ªæ¦‚ç‡å€¼ï¼Œæ‰€ä»¥æœ€å¥½ä½¿ç”¨ binary_crossentropyï¼ˆäºŒå…ƒäº¤å‰ç†µï¼‰ï¼Œä¼˜åŒ–å™¨åœ¨è¿™é‡Œé€‰æ‹© rmspropã€‚\n\n## éªŒè¯æ¨¡å‹\n\nç°åœ¨å–è®­ç»ƒæ•°æ®çš„å‰ 10000 æ¡ä½œä¸ºè®­ç»ƒï¼Œå¹¶å–å 10000 æ¡åšéªŒè¯æ•°æ®ï¼Œ20 ä¸ªè½®æ¬¡ï¼Œæ¯æ¬¡ä½¿ç”¨ 512 ä¸ªæ ·æœ¬è®­ç»ƒã€‚\n\n```py\nx_val = x_train[:10000]\npartial_x_train = x_train[10000:]\n\ny_val = y_train[:10000]\npartial_y_train = y_train[10000:]\n\nhistory = model.fit(\n  partial_x_train,\n  partial_y_train,\n  epochs=20,\n  batch_size=512,\n  validation_data=(x_val, y_val)\n)\n```\n\næˆ‘ä»¬å¯ä»¥ä½¿ç”¨ history è¿™ä¸ªå˜é‡ç»˜åˆ¶æ¯æ¬¡è®­ç»ƒçš„ç²¾ç¡®åº¦ä»¥åŠæŸå¤±å€¼å˜åŒ–ã€‚è®­ç»ƒçš„æŸå¤±ä¼šéšç€è½®æ¬¡å‡å°‘ï¼Œè€ŒéªŒè¯åˆ™ä¸ç„¶ï¼Œå¤§æ¦‚è®­ç»ƒåˆ°ç¬¬ 4 æ¬¡èƒ½å¤Ÿæ‹¿åˆ°ç†æƒ³çš„ç»“æœã€‚\n\n## é¢„æµ‹ç»“æœ\n\n```py\nmodel = models.Sequential()\nmodel.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\nmodel.add(layers.Dense(16, activation='relu'))\nmodel.add(layers.Dense(1,  activation='sigmoid'))\n\nmodel.compile(\n    optimizer='rmsprop',\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\nmodel.fit(\n    x_train,\n    y_train,\n    epochs=4,\n    batch_size=512\n)\nresults = model.evaluate(x_test, y_test) # [0.3345052400302887, 0.8581200242042542]\n\nmodel.predict(x_test)\n```\n\nä½¿ç”¨`predict`æ—¢å¯é¢„æµ‹æµ‹è¯•æ•°æ®çš„æ­£è´Ÿåé¦ˆäº†ã€‚\n\n```py\narray([[0.3796336 ],\n       [0.996838  ],\n       [0.667047  ],\n       ...,\n       [0.11539856],\n       [0.14184406],\n       [0.47513008]], dtype=float32)\n```\n\n## å¤šåˆ†ç±»é—®é¢˜\n\nä¹¦ä¸­è¿˜ä»‹ç»äº†è·¯é€ç¤¾æ–°é—»åˆ†ç±»ï¼Œä¸åŒç‚¹æ˜¯æ„å»ºç½‘ç»œæ—¶é€‰ç”¨å•å…ƒæ¯”è¾ƒå¤šï¼ˆä¹¦ä¸­é€‰æ‹© 64 ç»´ï¼Œå› ä¸ºæœ‰ 46 ä¸ªåˆ†ç±»ï¼‰ï¼Œæœ€åä¸€å±‚æ¿€æ´»å‡½æ•°é€‰æ‹©`softmax`å‡½æ•°ï¼Œå®ƒèƒ½ä¿è¯è¿™ 46 ä¸ªç±»çš„æ¦‚ç‡å’Œä¸º 1.\n\næœ€ç»ˆç¼–è¯‘æ—¶åº”é€‰æ‹©`categorical_crossentropy`åšæŸå¤±å‡½æ•°ã€‚\n","collection":"blog","data":{"type":"post","category":"book","tag":["tensorflow","keras","wsl2","python"],"series":{"slug":"learn-tensorflow","name":"å¼ é‡éº»è¾£çƒ«"}}},{"id":"2020/06/10/Keras.md","slug":"2020/06/10/keras","body":"\n# Keras å›å½’é—®é¢˜-é¢„æµ‹æˆ¿ä»·\n\nå›å½’é—®é¢˜ç”¨äºé¢„æµ‹ä¸€ä¸ªè¿ç»­å€¼è€Œä¸æ˜¯ç¦»æ•£å€¼ï¼Œå¦‚é¢„æµ‹æ˜å¤©æ°”æ¸©æˆ–è€…è½¯ä»¶å®Œæˆéœ€è¦çš„æ—¶é—´ã€‚\n\nè¿™ä¸ªä¾‹å­æ˜¯è¦é¢„æµ‹ 20 ä¸–çºª 70 å¹´ä»£ä¸­æœŸæ³¢å£«é¡¿éƒŠåŒºæˆ¿ä»·çš„ä¸­ä½æ•°ã€‚\n\n## è·å–æ•°æ®\n\n```py\nfrom keras.datasets import boston_housing\n\n(train_data, train_targets), (test_data, test_targets) =  boston_housing.load_data()\n```\n\nå¾—åˆ°çš„è®­ç»ƒæ•°æ®`train_data`æ˜¯ä¸€ä¸ª 404x13 å¼ é‡ï¼Œæµ‹è¯•æ•°æ®ä¸º 102x13 å¼ é‡ï¼Œè¿™ 13 é¡¹åŒ…æ‹¬ï¼š\n\n- æ€»çŠ¯ç½ªç‡\n- è¶…è¿‡ 25000 å¹³æ–¹è‹±å°ºçš„ä½å®…æ¯”ä¾‹\n- éå•†ç”¨åœ°æ¯”ä¾‹\n- charles river å˜é‡ï¼ˆ1 æˆ– 0ï¼‰\n- æ°®æ°§åŒ–ç‰©æµ“åº¦\n- å…¬å¯“æˆ¿é—´æ•°\n- å»ºäº 1940 å¹´å‰å»ºç­‘çš„å æœ‰é‡\n- åˆ°äº”ä¸ªæ³¢å£«é¡¿å°±ä¸šä¸­å¿ƒçš„åŠ æƒå¹³å‡å€¼\n- æ”¾å°„çŠ¶å…¬è·¯å¯è¾¾æŒ‡æ•°\n- æ¯ \\$1000 çš„ç‰©ä¸šç¨\n- åŸé•‡å­¦ç”Ÿæ•™å¸ˆæ¯”ä¾‹\n- 1000(Bk â€“ 0.63)^2ï¼ŒåŸé•‡é»‘äººå æœ‰ç‡\n- ä½æ”¶å…¥äººç¾¤å æœ‰ç‡\n\n```py\ntrain_data[0]\n# array([-0.27224633, -0.48361547, -0.43576161, -0.25683275, -0.1652266 ,\n#       -0.1764426 ,  0.81306188,  0.1166983 , -0.62624905, -0.59517003,\n#        1.14850044,  0.44807713,  0.8252202 ])\n```\n\n`train_targets` å¯¹åº”æˆ¿ä»·ï¼Œå•ä½ä¸ºåƒç¾å…ƒã€‚\n\n```py\ntrain_targets\n# array([15.2, 42.3, 50. , 21.1, 17.7, 18.5, 11.3, 15.6, 15.6, 14.4, 12.1,...\n```\n\n## å‡†å¤‡æ•°æ®\n\nå¯¹äºè¿™ 13 ç§æ•°æ®ï¼Œå–å€¼èŒƒå›´ä¸ç»Ÿä¸€ï¼Œè¿™é‡Œè¦åšå–å€¼çš„æ ‡å‡†åŒ–ï¼Œç®€å•ä¸Šè®²è®©æ¯ä¸ªç‰¹å¾å‡å»å…¶å¹³å‡å€¼å¹¶é™¤ä»¥æ ‡å‡†å·®ï¼Œè¿™æ ·ï¼Œæ¯ä¸ªç‰¹å¾å€¼éƒ½ä¼šå–å€¼äº 0 å·¦å³ï¼Œå’Œç»Ÿä¸€çš„æ ‡å‡†å·®ã€‚\n\n```py\nmean = train_data.mean(axis=0)\ntrain_data -= mean\nstd = train_data.std(axis=0)\ntrain_data /= std\n\ntest_data -= mean\ntest_data /= std\n```\n\nè¿™é‡Œä½¿ç”¨è®­ç»ƒæ•°æ®çš„å¹³å‡å€¼ï¼Œè¿™æ˜¯ä¸å¯¹çš„ï¼Œä½†æ˜¯ä¹¦é‡Œé¢å°±è¿™ä¹ˆå†™ï¼Œæˆ‘é ã€‚\n\n## åˆ›å»ºç½‘ç»œ\n\n```py\nfrom keras import models\nfrom keras import layers\n\ndef build_model():\n    model = models.Sequential()\n    model.add(layers.Dense(64, activation='relu',\n                          input_shape=(train_data.shape[1],)))\n    model.add(layers.Dense(64, activation='relu'))\n    model.add(layers.Dense(1))\n    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n    return model\n```\n\nç½‘ç»œæœ€åä¸€å±‚åªæœ‰ä¸€ä¸ªå•å…ƒï¼Œæ²¡æœ‰æ¿€æ´»ï¼Œè¿™æ˜¯æ ‡é‡å›å½’ã€‚æœ€åä¸€å±‚çš„æ¿€æ´»å‡½æ•°ç”¨äºé™å®šè¾“å‡ºå€¼çš„èŒƒå›´ï¼Œå¦‚æœæ˜¯ sigmoid å‡½æ•°ï¼Œåˆ™è¾“å‡º 0-1 çš„å€¼ï¼Œå¦‚æœæœ€åä¸€å±‚ä¸ºçº¯çº¿æ€§å±‚ï¼Œåˆ™å¯ä»¥é¢„æµ‹ä»»æ„èŒƒå›´çš„å€¼ã€‚\n\nç¼–è¯‘ç½‘ç»œä½¿ç”¨ mse æŸå¤±å‡½æ•°ï¼Œå³å‡æ–¹è¯¯å·®ï¼ˆmean squared errorï¼‰ï¼Œé¢„æµ‹å€¼å’Œç›®æ ‡å€¼å·®çš„å¹³æ–¹ã€‚è®­ç»ƒçš„æŒ‡æ ‡ä¸º mae å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆmean absolute errorï¼‰ï¼Œç›®æ ‡å€¼å’Œé¢„æµ‹å€¼å·®çš„ç»å¯¹å€¼ã€‚å¦‚æœ MAE å€¼ä¸º 0.5 åˆ™ä»£è¡¨é¢„æµ‹æˆ¿ä»·ä¸å®é™…æˆ¿ä»·å¹³å‡ç›¸å·® 500 ç¾å…ƒã€‚\n\n## åˆ©ç”¨ K æŠ˜éªŒè¯\n\nå› ä¸ºæ•°æ®é›†æ¯”è¾ƒå°ï¼Œå¯ä»¥ä½¿ç”¨ K æŠ˜éªŒè¯ï¼Œå³æŠŠæ•°æ®åˆ†ä¸ºå‡ åˆ†åŒºï¼ˆä¸€èˆ¬ 4~5 ç»„ï¼‰ï¼Œæœ€ç»ˆå–å‡ ä¸ªåˆ†åŒºçš„å¹³å‡å€¼ã€‚\n\n```py\nimport numpy as np\n\nk = 4\nnum_val_samples = len(train_data) // k\nnum_epochs = 100\nall_scores = []\nfor i in range(k):\n    print('processing fold #', i)\n    # Prepare the validation data: data from partition # k\n    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n\n    # Prepare the training data: data from all other partitions\n    partial_train_data = np.concatenate(\n        [train_data[:i * num_val_samples],\n         train_data[(i + 1) * num_val_samples:]],\n        axis=0)\n    partial_train_targets = np.concatenate(\n        [train_targets[:i * num_val_samples],\n         train_targets[(i + 1) * num_val_samples:]],\n        axis=0)\n\n    # Build the Keras model (already compiled)\n    model = build_model()\n    # Train the model (in silent mode, verbose=0)\n    model.fit(partial_train_data, partial_train_targets,\n              epochs=num_epochs, batch_size=1, verbose=0)\n    # Evaluate the model on the validation data\n    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n    all_scores.append(val_mae)\n```\n\nè¿™æ ·è·å¾—å€¼å¦‚ä¸‹ï¼Œ2.1~2.6 ä¸ç­‰ã€‚\n\n```py\nall_scores\n# [2.1905605792999268,\n#  2.4371392726898193,\n#  2.3653202056884766,\n#  2.5255486965179443]\n```\n\nå¦‚æœæƒ³è®©æ•°æ®æ›´ç²¾ç¡®ï¼Œä¸å¦‚è®©è®­ç»ƒæ¬¡æ•°ä» 100 å¢åŠ åˆ° 500ï¼Œä½†æ˜¯ä¹¦é‡Œé¢æœ€ç»ˆçš„æ•°æ®åˆ° 80 å°±å·®ä¸å¤šè¿‡æ‹Ÿåˆäº†ï¼Œå†åŠ ä¸Šæˆ‘çš„ç¬”è®°æœ¬è·‘ä¸èµ·æ¥ 500 æ¬¡ï¼Œè¿™é‡Œå°±ç®—äº†å§ã€‚ã€‚ã€‚\n\n## é¢„æµ‹\n\n```py\nmodel = build_model()\nmodel.fit(train_data, train_targets,\n          epochs=80, batch_size=16, verbose=0)\ntest_mse_score, test_mae_score = model.evaluate(test_data, test_targets)\n```\n\næœ€åé¢„æµ‹çš„æˆ¿ä»·å’Œç°å®æˆ¿ä»·ç›¸å·®å¤§æ¦‚ 2714 ç¾åˆ€ï¼ˆä¹¦é‡Œé¢„æµ‹çš„æ˜¯ 2550ï¼‰.\n","collection":"blog","data":{"type":"post","category":"book","tag":["tensorflow","keras","wsl2","python"],"series":{"slug":"learn-tensorflow","name":"å¼ é‡éº»è¾£çƒ«"}}},{"id":"2020/08/10/HTTP2.md","slug":"2020/08/10/http2","body":"\n# HTTP/2 åŸºç¡€æ•™ç¨‹\n\nè¿™æ˜¯åœ¨æ˜†æ˜çº¹èº«çš„æ—¶å€™ï¼Œå®åœ¨æ— èŠç¿»å‡ºæ¥å¹³æ¿è¯»çš„ä¹¦åšç¬”è®°ã€‚\n\n## http1 çš„é—®é¢˜\n\né˜Ÿå¤´é˜»å¡\ntcp åˆ©ç”¨ä½æ•ˆ\næ¶ˆæ¯å¤´éƒ¨è‡ƒè‚¿\nä¼˜å…ˆçº§è®¾ç½®å—é™\nç¬¬ä¸‰æ–¹èµ„æº\n\n## http2 å¯¹äº http1.1 çš„å˜åŒ–\n\näºŒè¿›åˆ¶åè®®\né¦–éƒ¨å‹ç¼©\nå¤šè·¯å¤ç”¨\nåŠ å¯†ä¼ è¾“\næ¨é€\n\n## http/2 çš„åæ¨¡å¼\n\nå› ä¸º http2 å¯¹æ¯” http1 çš„æ”¹åŠ¨å¾ˆå¤šï¼Œå¯¼è‡´æœ‰ä¸€äº›é’ˆå¯¹ http1 çš„ä¼˜åŒ–ï¼Œæ¯”å¦‚åˆ†æ•£å¤šä¸ªåŸŸååˆ†å‘èµ„æºåœ¨ http2 ä¸­ä¸ä¼šè¢«è§†ä¸ºä¼˜åŒ–ï¼ŒåŒæ—¶é›ªç¢§å›¾ä¹Ÿä¸å†æœ‰ä¼˜åŒ–ä½œç”¨ã€‚\n\n## æœªæ¥çš„ http\n\nquicï¼Œtcp å¯¼å‘ udp\n","collection":"blog","data":{"type":"post","category":"book"}}]}