{"posts":[{"id":"2020/01/13/Website Scraping with Python.md","slug":"2020/01/13/website-scraping-with-python","body":"\n# Website Scraping with Python\n\n## Parsing robots.txt\n\npage 28\n\n## Using Beautiful Soup\n\n用来解析 HTML\n\n- page 56 - 使用\n- page 101 - 利用 strainer 只解析想要的数据\n\n## Exporting the Data\n\n- page 80 - CSV\n- page 87 - JSON\n- page 90 - SQLite\n- page 97 - MongoDB\n\n## Using Scrapy\n\n- page 111\n\n## Handling JavaScript\n\n- page 186 - Splash\n- page 196 - Selenium\n\n## Cloud\n\n- page 206 - [Scrapy Cloud](https://scrapinghub.com/scrapy-cloud)\n  - page 211 - [mlab](https://mlab.com/)\n- page 216 - [PythonAnywhere](https://www.pythonanywhere.com/)\n","collection":"blog","data":{"type":"post","category":"book"}},{"id":"2020/03/09/JS.md","slug":"2020/03/09/js","body":"\n# 读完「你不知道的 JS」第一版\n\n![book cover](./you-dont-know-about-JS.jpg)\n\n「你不知道的 JS」是[getify](https://github.com/getify)的一本关于深究 JavaScript 的书，中文版分为三册，前一阵子发现作者正在更新第二版，读了几页真的不错，于是找来第一版读了下。\n\n第一版第一册就是深究词法、上下文，这些东西大部分书都提的比较少，说实话，虽然我已经了解了，但是总是忘。这本书第二版比第一版好太多了，有兴趣的可以去读一下。\n\n第二册，异步，关于 promise 和生成器，应该是现有的关于异步写的最详细的书了，不过我更感兴趣他能更深度讲解一下 observable，虽然这个还不是 JavaScript 的标准。\n\n第三册，ES6 相关，很有趣的是，真本书前半部分都是关于第一第二册的简述，所以如果你刚好上不了网只能买书的话，直接看这一本也差不多。\n\n后面我可能会读一下书中推荐的[Functiional Light JS](https://github.com/getify/Functional-Light-JS)，以及 2ality 写的[Deep JavaScript: Theory and techniques](https://exploringjs.com/deep-js/)。\n","collection":"blog","data":{"type":"post","category":"book","tag":["JavaScript"]}},{"id":"2020/03/21/Functional-Light-JS.md","slug":"2020/03/21/functional-light-js","body":"\n比较简单的一本[开源书](https://github.com/getify/Functional-Light-JS)，大概算函数式编程的入门书+JavaScript 部分介绍了。\n\n推荐章节：\n\n- 第四章，为什么要减少函数参数以及柯里化，这样有利于使用 compose 函数组合步骤。\n- 第八章，递归，利用 es6 上面的尾递归调用提高代码可读性。\n- 附录 A，提高性能，减少运算损耗。\n","collection":"blog","data":{"type":"post","category":"book","tag":["JavaScript","functinal-programming"]}},{"id":"2020/04/13/threejs-fundamentals-threejs.md","slug":"2020/04/13/threejs-fundamentals-threejs","body":"\n# threejs fundamentals 名副其实的 threejs 入门书\n\n最近几天真是忙，除了读这本书，我的平板（酷比魔方 Mix Plus）突然进不去系统了，查了一下，原来山寨本硬盘质量不好，莫名其妙全清了（惊！），好在做了备份，当然因为不想花钱，没再买一个 SATA SSD 硬盘，花了一些时间重做系统。\n\n祸不单行，主力机 Dell inspiron 13 风扇坏了，虽然已经报修，但还在走流程，这使得我现在只能以 1.8GHz（关闭风扇） 工作 😭。\n\n正好，刚做完系统，所以这篇文章我争取在平板上写，以保证系统安装正确。\n\n另外，因为抢到了福田的消费券，所以这本书的实现，争取在本周完成。\n","collection":"blog","data":{"type":"post","category":"book","tag":["threejs","JavaScript","webGL"]}},{"id":"2020/06/06/Python.md","slug":"2020/06/06/python","body":"\n# 读「Python 深度学习」数学基础\n\n- 「学习」是指找到一组模型参数，使给定的训练数据样本和对应目标上的损失函数最小化。\n- 学习的过程：随机取包含数据样本及其目标值的批量，并计算批量损失相对于网络参数的梯度（梯度可以理解为对于张量计算的倒数）。随后将网络参数沿着梯度的反方向稍稍移动（移动距离由学习率指定）。\n- 整个学习过程之所以能够实现，是因为神经网络是一系列可微分的张量运算，因此可以使用求导的链式法则来得到梯度函数，这个函数将当前参数和当前数据批量映射为一个梯度值。\n- 「损失」是训练过程中需要最小化的量，因此它能够衡量当前任务是否已经成功解决。\n- 「优化器」是损失梯度更新参数的具体方式，比如 RMSProp 优化器，带动量的随机梯度下降（SGD）等。\n","collection":"blog","data":{"type":"post","category":"book","tag":["tensorflow"],"series":{"slug":"learn-tensorflow","name":"张量麻辣烫"}}},{"id":"2020/06/07/WSL2-Keras.md","slug":"2020/06/07/wsl2-keras","body":"\n# 在 WSL2 安装 Keras 及其依赖\n\n## 安装 python\n\n首先安装 python3 和 pip3，并且使用 USTC 源。\n\n```shell\nsudo apt install python3 python3-pip python3-dev\npip3 install -i https://mirrors.ustc.edu.cn/pypi/web/simple pip -U\npip3 config set global.index-url https://mirrors.ustc.edu.cn/pypi/web/simple\n```\n\n## 安装 python 科学套件\n\n1. 安装 BLAS 库（OpenBLAS），确保可以在 CPU 上面做张量运算。\n\n   ```shell\n   sudo apt install build-essential cmake git unzip pkg-config libopenblas-dev liblapack-dev\n   ```\n\n2. 安装 Python 科学套件：Numpy、SciPy 和 Matplotlib。这是做科学计算必须的。\n\n   ```shell\n    sudo apt install python3-numpy python3-scipy python3-matplotlib python3-yaml\n   ```\n\n3. 安装 HDF5，最初由 NASA 开发，用于保存数值数据的大文件。它可以帮助 Keras 模型快速高效保存到磁盘。\n\n   ```shell\n    sudo apt install libhdf5-serial-dev python3-h5py\n   ```\n\n4. 安装 Graphviz 和 pydot-ng，这两个包用于可视化 Keras 模型。并不是必须的。\n\n   ```shell\n    sudo apt install graphviz\n    pip3 install pydot-ng\n   ```\n\n5. 安装 tensorflow\n\n   ```shell\n   pip3 install tensorflow\n   pip3 install tensorflow-gpu\n   ```\n\n## 安装 Keras\n\n使用 pip3 安装 Keras。\n\n```shell\npip3 install keras\n```\n\n可以从 GitHub 中下载 Keras 的例子\n\n```shell\ngit clone https://github.com/keras-team/keras.git\n```\n\n测试一个例子\n\n```shell\npython3 mnist_cnn.py\n```\n\n执行成功后会在`~/.keras/keras.json`生成配置文件。\n","collection":"blog","data":{"type":"post","category":"book","tag":["tensorflow","keras","wsl2","python"],"series":{"slug":"learn-tensorflow","name":"张量麻辣烫"}}},{"id":"2020/06/08/Keras.md","slug":"2020/06/08/keras","body":"\n# Keras 分类问题-电影评论分类\n\n> 「Python 深度学习」的例子在[GitHub](https://github.com/fchollet/deep-learning-with-python-notebooks)上。\n> 别问我为啥不写在 notebook 上，等我心情好就给网站加一下这给你功能吧\n\n本文的 notebook 在[这个连接](https://github.com/gongbaodd/keras_study/blob/master/3.5%20movie%20reviews.ipynb)。\n\n将 IMDB 上的 50000 条两极分化的评论，一般用于训练，一半用于测试。这些数据中 data 是词索引组成的二维数组，每一行对应每条评论，label 为 0\\1 数组，0 表示负面评论，1 表示积极评论。\n\n## 处理数据\n\n```py\nimport keras\nfrom keras.datasets import imdb\n\n(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = 1000)\n```\n\n数据集里面的数字可以解析成评论，每一个`data`都是如`[0, 14, 32, 56...]`的数组。\n\n```py\nword_index = imdb.get_word_index()\nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\ndecoded_review = ' '.join([reverse_word_index.get(i-3, '?') for i in train_data[0]])\n```\n\n使用以下方法即可以把训练数据处理成 0/1 二维张量。\n\n```py\nimport numpy as np\n\ndef vectorize_sequences(sequences, dimension=10000):\n    results = np.zeros((len(sequences), dimension))\n    for i, sequence in enumerate(sequences):\n        results[i, sequence] = 1\n    return results\n\nx_train = vectorize_sequences(train_data)\nx_test = vectorize_sequesnces(test_data)\n\ny_train = np.array(train_labels).astype('float32')\ny_test = np.array(test_labels).astype('float32')\n```\n\n简单地说，x 值为 25000\\*10000 的二维数组，某个单词出现则在它所在的评论序号和对应的单词索引的位置上赋值 1，而 y 对应每个 x 行评论的正负反馈。\n\n## 构建网络\n\n使用 rlu 激活（线性整流函数 `max(0, input)`）的全链接层(Dense)就能很好处理这种输入值为标量和向量的情况，如`Dense(16, activation='relu')`。前面的 16 是一个隐藏单元，表示会将数据表示在一个 16 维的表示空间中，隐藏单元越高，能够学到的网络越复杂，计算代价也越大，但并不是越高越好，往往高了会学到不正确的模式。\n\n两个中间网络，第三层使用 sigmoid 激活（s 函数）以输出 0~1 的值\n\n![S函数](https://wikimedia.org/api/rest_v1/media/math/render/svg/a26a3fa3cbb41a3abfe4c7ff88d47f0181489d13)\n\n```py\nfrom keras import models\nfrom keras import layers\nfrom keras import optimizers\nfrom keras import losses\nfrom keras import metrics\n\nmodel = models.Sequential()\nmodel.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\nmodel.add(layers.Dense(16, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer=optimizers.RMSprop(lr=0.001),\n              loss=losses.binary_crossentropy,\n              metrics=[metrics.binary_accuracy])\n```\n\n编译网络需要选择损失函数和优化器，这里是个二分类问题，网络最终输出一个概率值，所以最好使用 binary_crossentropy（二元交叉熵），优化器在这里选择 rmsprop。\n\n## 验证模型\n\n现在取训练数据的前 10000 条作为训练，并取后 10000 条做验证数据，20 个轮次，每次使用 512 个样本训练。\n\n```py\nx_val = x_train[:10000]\npartial_x_train = x_train[10000:]\n\ny_val = y_train[:10000]\npartial_y_train = y_train[10000:]\n\nhistory = model.fit(\n  partial_x_train,\n  partial_y_train,\n  epochs=20,\n  batch_size=512,\n  validation_data=(x_val, y_val)\n)\n```\n\n我们可以使用 history 这个变量绘制每次训练的精确度以及损失值变化。训练的损失会随着轮次减少，而验证则不然，大概训练到第 4 次能够拿到理想的结果。\n\n## 预测结果\n\n```py\nmodel = models.Sequential()\nmodel.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\nmodel.add(layers.Dense(16, activation='relu'))\nmodel.add(layers.Dense(1,  activation='sigmoid'))\n\nmodel.compile(\n    optimizer='rmsprop',\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\nmodel.fit(\n    x_train,\n    y_train,\n    epochs=4,\n    batch_size=512\n)\nresults = model.evaluate(x_test, y_test) # [0.3345052400302887, 0.8581200242042542]\n\nmodel.predict(x_test)\n```\n\n使用`predict`既可预测测试数据的正负反馈了。\n\n```py\narray([[0.3796336 ],\n       [0.996838  ],\n       [0.667047  ],\n       ...,\n       [0.11539856],\n       [0.14184406],\n       [0.47513008]], dtype=float32)\n```\n\n## 多分类问题\n\n书中还介绍了路透社新闻分类，不同点是构建网络时选用单元比较多（书中选择 64 维，因为有 46 个分类），最后一层激活函数选择`softmax`函数，它能保证这 46 个类的概率和为 1.\n\n最终编译时应选择`categorical_crossentropy`做损失函数。\n","collection":"blog","data":{"type":"post","category":"book","tag":["tensorflow","keras","wsl2","python"],"series":{"slug":"learn-tensorflow","name":"张量麻辣烫"}}},{"id":"2020/06/10/Keras.md","slug":"2020/06/10/keras","body":"\n# Keras 回归问题-预测房价\n\n回归问题用于预测一个连续值而不是离散值，如预测明天气温或者软件完成需要的时间。\n\n这个例子是要预测 20 世纪 70 年代中期波士顿郊区房价的中位数。\n\n## 获取数据\n\n```py\nfrom keras.datasets import boston_housing\n\n(train_data, train_targets), (test_data, test_targets) =  boston_housing.load_data()\n```\n\n得到的训练数据`train_data`是一个 404x13 张量，测试数据为 102x13 张量，这 13 项包括：\n\n- 总犯罪率\n- 超过 25000 平方英尺的住宅比例\n- 非商用地比例\n- charles river 变量（1 或 0）\n- 氮氧化物浓度\n- 公寓房间数\n- 建于 1940 年前建筑的占有量\n- 到五个波士顿就业中心的加权平均值\n- 放射状公路可达指数\n- 每 \\$1000 的物业税\n- 城镇学生教师比例\n- 1000(Bk – 0.63)^2，城镇黑人占有率\n- 低收入人群占有率\n\n```py\ntrain_data[0]\n# array([-0.27224633, -0.48361547, -0.43576161, -0.25683275, -0.1652266 ,\n#       -0.1764426 ,  0.81306188,  0.1166983 , -0.62624905, -0.59517003,\n#        1.14850044,  0.44807713,  0.8252202 ])\n```\n\n`train_targets` 对应房价，单位为千美元。\n\n```py\ntrain_targets\n# array([15.2, 42.3, 50. , 21.1, 17.7, 18.5, 11.3, 15.6, 15.6, 14.4, 12.1,...\n```\n\n## 准备数据\n\n对于这 13 种数据，取值范围不统一，这里要做取值的标准化，简单上讲让每个特征减去其平均值并除以标准差，这样，每个特征值都会取值于 0 左右，和统一的标准差。\n\n```py\nmean = train_data.mean(axis=0)\ntrain_data -= mean\nstd = train_data.std(axis=0)\ntrain_data /= std\n\ntest_data -= mean\ntest_data /= std\n```\n\n这里使用训练数据的平均值，这是不对的，但是书里面就这么写，我靠。\n\n## 创建网络\n\n```py\nfrom keras import models\nfrom keras import layers\n\ndef build_model():\n    model = models.Sequential()\n    model.add(layers.Dense(64, activation='relu',\n                          input_shape=(train_data.shape[1],)))\n    model.add(layers.Dense(64, activation='relu'))\n    model.add(layers.Dense(1))\n    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n    return model\n```\n\n网络最后一层只有一个单元，没有激活，这是标量回归。最后一层的激活函数用于限定输出值的范围，如果是 sigmoid 函数，则输出 0-1 的值，如果最后一层为纯线性层，则可以预测任意范围的值。\n\n编译网络使用 mse 损失函数，即均方误差（mean squared error），预测值和目标值差的平方。训练的指标为 mae 平均绝对误差（mean absolute error），目标值和预测值差的绝对值。如果 MAE 值为 0.5 则代表预测房价与实际房价平均相差 500 美元。\n\n## 利用 K 折验证\n\n因为数据集比较小，可以使用 K 折验证，即把数据分为几分区（一般 4~5 组），最终取几个分区的平均值。\n\n```py\nimport numpy as np\n\nk = 4\nnum_val_samples = len(train_data) // k\nnum_epochs = 100\nall_scores = []\nfor i in range(k):\n    print('processing fold #', i)\n    # Prepare the validation data: data from partition # k\n    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n\n    # Prepare the training data: data from all other partitions\n    partial_train_data = np.concatenate(\n        [train_data[:i * num_val_samples],\n         train_data[(i + 1) * num_val_samples:]],\n        axis=0)\n    partial_train_targets = np.concatenate(\n        [train_targets[:i * num_val_samples],\n         train_targets[(i + 1) * num_val_samples:]],\n        axis=0)\n\n    # Build the Keras model (already compiled)\n    model = build_model()\n    # Train the model (in silent mode, verbose=0)\n    model.fit(partial_train_data, partial_train_targets,\n              epochs=num_epochs, batch_size=1, verbose=0)\n    # Evaluate the model on the validation data\n    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n    all_scores.append(val_mae)\n```\n\n这样获得值如下，2.1~2.6 不等。\n\n```py\nall_scores\n# [2.1905605792999268,\n#  2.4371392726898193,\n#  2.3653202056884766,\n#  2.5255486965179443]\n```\n\n如果想让数据更精确，不如让训练次数从 100 增加到 500，但是书里面最终的数据到 80 就差不多过拟合了，再加上我的笔记本跑不起来 500 次，这里就算了吧。。。\n\n## 预测\n\n```py\nmodel = build_model()\nmodel.fit(train_data, train_targets,\n          epochs=80, batch_size=16, verbose=0)\ntest_mse_score, test_mae_score = model.evaluate(test_data, test_targets)\n```\n\n最后预测的房价和现实房价相差大概 2714 美刀（书里预测的是 2550）.\n","collection":"blog","data":{"type":"post","category":"book","tag":["tensorflow","keras","wsl2","python"],"series":{"slug":"learn-tensorflow","name":"张量麻辣烫"}}},{"id":"2020/08/10/HTTP2.md","slug":"2020/08/10/http2","body":"\n# HTTP/2 基础教程\n\n这是在昆明纹身的时候，实在无聊翻出来平板读的书做笔记。\n\n## http1 的问题\n\n队头阻塞\ntcp 利用低效\n消息头部臃肿\n优先级设置受限\n第三方资源\n\n## http2 对于 http1.1 的变化\n\n二进制协议\n首部压缩\n多路复用\n加密传输\n推送\n\n## http/2 的反模式\n\n因为 http2 对比 http1 的改动很多，导致有一些针对 http1 的优化，比如分散多个域名分发资源在 http2 中不会被视为优化，同时雪碧图也不再有优化作用。\n\n## 未来的 http\n\nquic，tcp 导向 udp\n","collection":"blog","data":{"type":"post","category":"book"}}]}