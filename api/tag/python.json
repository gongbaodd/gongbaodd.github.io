{"posts":[{"id":"2020/02/04/Windows-scrapy.md","slug":"2020/02/04/windows-scrapy","body":"\n# Windows 上使用 scrapy 抓取网页\n\n过去一周，我在尝试在 Windows 上面使用 python，我会在这一篇文章中总结一下这一次体验的经验，代码已经发布到[GitHub](https://github.com/gongbaodd/webScrapingStudy)上面。\n\n## 安装 python\n\n本身 python 的版本就比较混乱，Windows 又提供了商店版，而且 WSL 下面也可以安装 Linux 的 python，我都体验了一下。\n\n- Windows 商店版，这个貌似就是为了教学使用，因为 Windows 目前比较尴尬，全局安装的包可能会有兼容性问题，但是因为商店版都运行在沙盒之下，基本上就没多少修改的可能了。\n- WSL 版本，这个版本体验的是纯正的 Linux，但是一定要注意，如果没安装 Xserver 就相当于没有图形界面。\n- x64 版本，这个问题在于安装文件的地址都跟了个 x64。\n- win32 版本，这个版本的问题比较小，除了 pyenv 需要单独下载 Windows 版和[jupyter 报错](https://gongbaodd.github.io/tech/2020/01/06/%E4%BF%AE%E5%A4%8DWindows%E4%B8%8B%E6%89%93%E5%BC%80Jupyter%E6%97%B6%E6%8A%A5NotImplementError.html)，还没碰到其他问题。\n\n## pyenv\n\n介于 python 大版本兼容性，个人认为要安装一个版本管理器。因为习惯于 JavaScript 工作环境，我肯定会寻找类似于 nvm 的映射就是 pyenv，在 Windows 下面可以通过 chocolatey 安装。\n\n```shell\nsudo choco install pyenv-win\n```\n\n下面几个命令是最常用的。\n\n- `pyenv install -l`查看可以安装的 python 版本号。\n- `pyenv local install 3.8.0`在项目中安装 3.8.0 版本（会在项目目录增加.python-version 文件）。\n- `pyenv version`查看现在的 python 版本。\n- `pyenv versions`查看安装过的 python 版本。\n\nwin10 上了一个新功能，控制台会引导 python 到应用商店，在“设置>应用和功能>应用执行名”中可以勾掉这个功能\n\n## virtualenv\n\npython 的包管理其实很差，都是放到 global 下面，这就导致多个项目可能都用同一个依赖。那么如何实现每个项目都有自己的依赖呢？这就靠 virtualenv。\n\n```shell\npip install virtualenv\n```\n\n如下命令最常用\n\n- `virtualenv [venv folder name]`新建虚拟环境文件夹。\n- `source [venv folder name]/Scripts/activate`启动虚拟环境（在 Linux 下面是 bin/activate）。\n- `deactivate`关闭虚拟环境（这个在 Linux 会比较常用）。\n\n## scrapy\n\nscrapy 是一个 python 的爬虫框架，使用 pip 可以安装 scrapy。\n\n```shell\npip install scrapy\n```\n\n下面是 scrapy 用的比较多的几个命令\n\n- `scrapy startproject [project name]`新建项目。\n- `scrapy crawl [spider name] -o [output file]`爬取页面并输出结果到文件。\n\nscrapy 的概念比较多，包括 spider、pipeline、middleware 等等，但个人看来基本上看完[tutorial](https://docs.scrapy.org/en/latest/intro/tutorial.html#our-first-spider)就可以上手了。\n\n### scrapy shell\n\n执行`scrapy shell [url]`可以以命令形式使用 scrapy。\n\n- `fetch('http://xxx.com')`爬取页面\n- `view(response)`浏览爬取的页面\n- `response.css('a::text').extract()`析取页面中链接的文字列表\n- `response.css('a::attr(href)')`析取页面中链接列表\n\n### 发起 Xmlhttp 请求\n\n使用[Scrapy.FormRequest](https://docs.scrapy.org/en/latest/topics/request-response.html#scrapy.http.FormRequest)发起请求，接收到结果可以使用[response.body_as_unicode()]解析 JSON 为字典。\n\n### splash\n\n截至目前，scrapy 都只能渲染非 JavaScript 运行的页面，但是借助 splash 就可以解析 JavaScript 了。我们使用 docker 可以尝试一下 splash。\n\n```shell\ndocker pull scrapinghub/splash\ndocker run -p 8050:8050 scrapinghub/splash\n```\n\n访问 localhost:8050 即可访问 splash。通过安装`scrapy-splash`可以在 scrapy 中使用 splash，具体安装步骤[官网](https://github.com/scrapy-plugins/scrapy-splash)已经很详细在此不做赘述。\n\n## Scrapinghub\n\nScrapinghub 是一个基于 scrapy 的云服务，可以将自己的爬虫部署到该平台。[这里](https://support.scrapinghub.com/support/solutions/articles/22000200667-running-a-scrapy-spider)有个工具可以帮助部署（当然通过链接 GitHub 可以做到 master 部署）。\n\n```shell\npip install shub\n```\n\n### 解决依赖\n\n爬虫上传到 Scrapinghub 之后，会部署失败，可能源于以下两点。\n\n- scrapinghub 使用的是 python2\n- 部份依赖没有安装\n\n以上两点可以通过修改 scrapinghub.yml 完成\n\n```yaml\nprojects:\n  default: 427692\nstacks:\n  default: scrapy:1.8-py3\nrequirements:\n  file: requirements.txt\n```\n\n通过`pip freeze`能够列举出目前环境下的所有包，需要挑出可能缺少的依赖写在 requirements.txt 里面（没错这一步只能人工完成，不要妄想把所有包都写进去）。\n\n```\nbeautifulsoup4==4.8.2\nfeedparser===5.2.1\nscrapy-splash==0.7.2\n```\n\n## 单元测试\n\n使用 python 自带的 unittest 模块以及 pytest 可以对代码进行单元测试。可以参考我代码中的[测试](https://github.com/gongbaodd/webScrapingStudy/tree/master/test/test_spider)。\n\n执行 pytest 的时候会出现找不到模块的问题，可以按照如下方式重置根地址位置。\n\n```\npython -m pytest [file path]\n```\n\n## 代码优化和格式化\n\n这里比较爽了，如果用的是 vscode，在第一次格式化代码的时候，vscode 就会安装格式化工具。\n\n## pre-commit\n\npre-commit 是一个 git 钩子工具，简单说，当本地代码不满足要求的时候，利用这个工具自动格式化代码或者阻止用户提交代码。可以参考[官网配置](https://pre-commit.com/)。\n\n## 包健康检查\n\n目前没在 python 找到一个类似于 yarn audit 的东西，到那时找到了一个[SNYK](https://snyk.io)是一个跨语言的包健康检查工具，但是貌似还有 bug，暂时先裸奔好了。\n\n## 持续集成\n\n目前我是用 Travis 做集成，配置文件可参考[此文件](https://github.com/gongbaodd/webScrapingStudy/blob/master/.travis.yml)。\n\n## 兼容性处理\n\n另外还找到一个 python 版本兼容测试工具，考虑到使用 python 命令的人自己的 python 版本并不确定，[tox](https://pypi.org/project/tox/)则是用来测试 py 是否兼容某些 python 的版本。\n","collection":"blog","data":{"type":"post","category":"tech","tag":["scrapy","python"]}},{"id":"2020/06/07/WSL2-Keras.md","slug":"2020/06/07/wsl2-keras","body":"\n# 在 WSL2 安装 Keras 及其依赖\n\n## 安装 python\n\n首先安装 python3 和 pip3，并且使用 USTC 源。\n\n```shell\nsudo apt install python3 python3-pip python3-dev\npip3 install -i https://mirrors.ustc.edu.cn/pypi/web/simple pip -U\npip3 config set global.index-url https://mirrors.ustc.edu.cn/pypi/web/simple\n```\n\n## 安装 python 科学套件\n\n1. 安装 BLAS 库（OpenBLAS），确保可以在 CPU 上面做张量运算。\n\n   ```shell\n   sudo apt install build-essential cmake git unzip pkg-config libopenblas-dev liblapack-dev\n   ```\n\n2. 安装 Python 科学套件：Numpy、SciPy 和 Matplotlib。这是做科学计算必须的。\n\n   ```shell\n    sudo apt install python3-numpy python3-scipy python3-matplotlib python3-yaml\n   ```\n\n3. 安装 HDF5，最初由 NASA 开发，用于保存数值数据的大文件。它可以帮助 Keras 模型快速高效保存到磁盘。\n\n   ```shell\n    sudo apt install libhdf5-serial-dev python3-h5py\n   ```\n\n4. 安装 Graphviz 和 pydot-ng，这两个包用于可视化 Keras 模型。并不是必须的。\n\n   ```shell\n    sudo apt install graphviz\n    pip3 install pydot-ng\n   ```\n\n5. 安装 tensorflow\n\n   ```shell\n   pip3 install tensorflow\n   pip3 install tensorflow-gpu\n   ```\n\n## 安装 Keras\n\n使用 pip3 安装 Keras。\n\n```shell\npip3 install keras\n```\n\n可以从 GitHub 中下载 Keras 的例子\n\n```shell\ngit clone https://github.com/keras-team/keras.git\n```\n\n测试一个例子\n\n```shell\npython3 mnist_cnn.py\n```\n\n执行成功后会在`~/.keras/keras.json`生成配置文件。\n","collection":"blog","data":{"type":"post","category":"book","tag":["tensorflow","keras","wsl2","python"],"series":{"slug":"learn-tensorflow","name":"张量麻辣烫"}}},{"id":"2020/06/08/Keras.md","slug":"2020/06/08/keras","body":"\n# Keras 分类问题-电影评论分类\n\n> 「Python 深度学习」的例子在[GitHub](https://github.com/fchollet/deep-learning-with-python-notebooks)上。\n> 别问我为啥不写在 notebook 上，等我心情好就给网站加一下这给你功能吧\n\n本文的 notebook 在[这个连接](https://github.com/gongbaodd/keras_study/blob/master/3.5%20movie%20reviews.ipynb)。\n\n将 IMDB 上的 50000 条两极分化的评论，一般用于训练，一半用于测试。这些数据中 data 是词索引组成的二维数组，每一行对应每条评论，label 为 0\\1 数组，0 表示负面评论，1 表示积极评论。\n\n## 处理数据\n\n```py\nimport keras\nfrom keras.datasets import imdb\n\n(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = 1000)\n```\n\n数据集里面的数字可以解析成评论，每一个`data`都是如`[0, 14, 32, 56...]`的数组。\n\n```py\nword_index = imdb.get_word_index()\nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\ndecoded_review = ' '.join([reverse_word_index.get(i-3, '?') for i in train_data[0]])\n```\n\n使用以下方法即可以把训练数据处理成 0/1 二维张量。\n\n```py\nimport numpy as np\n\ndef vectorize_sequences(sequences, dimension=10000):\n    results = np.zeros((len(sequences), dimension))\n    for i, sequence in enumerate(sequences):\n        results[i, sequence] = 1\n    return results\n\nx_train = vectorize_sequences(train_data)\nx_test = vectorize_sequesnces(test_data)\n\ny_train = np.array(train_labels).astype('float32')\ny_test = np.array(test_labels).astype('float32')\n```\n\n简单地说，x 值为 25000\\*10000 的二维数组，某个单词出现则在它所在的评论序号和对应的单词索引的位置上赋值 1，而 y 对应每个 x 行评论的正负反馈。\n\n## 构建网络\n\n使用 rlu 激活（线性整流函数 `max(0, input)`）的全链接层(Dense)就能很好处理这种输入值为标量和向量的情况，如`Dense(16, activation='relu')`。前面的 16 是一个隐藏单元，表示会将数据表示在一个 16 维的表示空间中，隐藏单元越高，能够学到的网络越复杂，计算代价也越大，但并不是越高越好，往往高了会学到不正确的模式。\n\n两个中间网络，第三层使用 sigmoid 激活（s 函数）以输出 0~1 的值\n\n![S函数](https://wikimedia.org/api/rest_v1/media/math/render/svg/a26a3fa3cbb41a3abfe4c7ff88d47f0181489d13)\n\n```py\nfrom keras import models\nfrom keras import layers\nfrom keras import optimizers\nfrom keras import losses\nfrom keras import metrics\n\nmodel = models.Sequential()\nmodel.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\nmodel.add(layers.Dense(16, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer=optimizers.RMSprop(lr=0.001),\n              loss=losses.binary_crossentropy,\n              metrics=[metrics.binary_accuracy])\n```\n\n编译网络需要选择损失函数和优化器，这里是个二分类问题，网络最终输出一个概率值，所以最好使用 binary_crossentropy（二元交叉熵），优化器在这里选择 rmsprop。\n\n## 验证模型\n\n现在取训练数据的前 10000 条作为训练，并取后 10000 条做验证数据，20 个轮次，每次使用 512 个样本训练。\n\n```py\nx_val = x_train[:10000]\npartial_x_train = x_train[10000:]\n\ny_val = y_train[:10000]\npartial_y_train = y_train[10000:]\n\nhistory = model.fit(\n  partial_x_train,\n  partial_y_train,\n  epochs=20,\n  batch_size=512,\n  validation_data=(x_val, y_val)\n)\n```\n\n我们可以使用 history 这个变量绘制每次训练的精确度以及损失值变化。训练的损失会随着轮次减少，而验证则不然，大概训练到第 4 次能够拿到理想的结果。\n\n## 预测结果\n\n```py\nmodel = models.Sequential()\nmodel.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\nmodel.add(layers.Dense(16, activation='relu'))\nmodel.add(layers.Dense(1,  activation='sigmoid'))\n\nmodel.compile(\n    optimizer='rmsprop',\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\nmodel.fit(\n    x_train,\n    y_train,\n    epochs=4,\n    batch_size=512\n)\nresults = model.evaluate(x_test, y_test) # [0.3345052400302887, 0.8581200242042542]\n\nmodel.predict(x_test)\n```\n\n使用`predict`既可预测测试数据的正负反馈了。\n\n```py\narray([[0.3796336 ],\n       [0.996838  ],\n       [0.667047  ],\n       ...,\n       [0.11539856],\n       [0.14184406],\n       [0.47513008]], dtype=float32)\n```\n\n## 多分类问题\n\n书中还介绍了路透社新闻分类，不同点是构建网络时选用单元比较多（书中选择 64 维，因为有 46 个分类），最后一层激活函数选择`softmax`函数，它能保证这 46 个类的概率和为 1.\n\n最终编译时应选择`categorical_crossentropy`做损失函数。\n","collection":"blog","data":{"type":"post","category":"book","tag":["tensorflow","keras","wsl2","python"],"series":{"slug":"learn-tensorflow","name":"张量麻辣烫"}}},{"id":"2020/06/10/Keras.md","slug":"2020/06/10/keras","body":"\n# Keras 回归问题-预测房价\n\n回归问题用于预测一个连续值而不是离散值，如预测明天气温或者软件完成需要的时间。\n\n这个例子是要预测 20 世纪 70 年代中期波士顿郊区房价的中位数。\n\n## 获取数据\n\n```py\nfrom keras.datasets import boston_housing\n\n(train_data, train_targets), (test_data, test_targets) =  boston_housing.load_data()\n```\n\n得到的训练数据`train_data`是一个 404x13 张量，测试数据为 102x13 张量，这 13 项包括：\n\n- 总犯罪率\n- 超过 25000 平方英尺的住宅比例\n- 非商用地比例\n- charles river 变量（1 或 0）\n- 氮氧化物浓度\n- 公寓房间数\n- 建于 1940 年前建筑的占有量\n- 到五个波士顿就业中心的加权平均值\n- 放射状公路可达指数\n- 每 \\$1000 的物业税\n- 城镇学生教师比例\n- 1000(Bk – 0.63)^2，城镇黑人占有率\n- 低收入人群占有率\n\n```py\ntrain_data[0]\n# array([-0.27224633, -0.48361547, -0.43576161, -0.25683275, -0.1652266 ,\n#       -0.1764426 ,  0.81306188,  0.1166983 , -0.62624905, -0.59517003,\n#        1.14850044,  0.44807713,  0.8252202 ])\n```\n\n`train_targets` 对应房价，单位为千美元。\n\n```py\ntrain_targets\n# array([15.2, 42.3, 50. , 21.1, 17.7, 18.5, 11.3, 15.6, 15.6, 14.4, 12.1,...\n```\n\n## 准备数据\n\n对于这 13 种数据，取值范围不统一，这里要做取值的标准化，简单上讲让每个特征减去其平均值并除以标准差，这样，每个特征值都会取值于 0 左右，和统一的标准差。\n\n```py\nmean = train_data.mean(axis=0)\ntrain_data -= mean\nstd = train_data.std(axis=0)\ntrain_data /= std\n\ntest_data -= mean\ntest_data /= std\n```\n\n这里使用训练数据的平均值，这是不对的，但是书里面就这么写，我靠。\n\n## 创建网络\n\n```py\nfrom keras import models\nfrom keras import layers\n\ndef build_model():\n    model = models.Sequential()\n    model.add(layers.Dense(64, activation='relu',\n                          input_shape=(train_data.shape[1],)))\n    model.add(layers.Dense(64, activation='relu'))\n    model.add(layers.Dense(1))\n    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n    return model\n```\n\n网络最后一层只有一个单元，没有激活，这是标量回归。最后一层的激活函数用于限定输出值的范围，如果是 sigmoid 函数，则输出 0-1 的值，如果最后一层为纯线性层，则可以预测任意范围的值。\n\n编译网络使用 mse 损失函数，即均方误差（mean squared error），预测值和目标值差的平方。训练的指标为 mae 平均绝对误差（mean absolute error），目标值和预测值差的绝对值。如果 MAE 值为 0.5 则代表预测房价与实际房价平均相差 500 美元。\n\n## 利用 K 折验证\n\n因为数据集比较小，可以使用 K 折验证，即把数据分为几分区（一般 4~5 组），最终取几个分区的平均值。\n\n```py\nimport numpy as np\n\nk = 4\nnum_val_samples = len(train_data) // k\nnum_epochs = 100\nall_scores = []\nfor i in range(k):\n    print('processing fold #', i)\n    # Prepare the validation data: data from partition # k\n    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n\n    # Prepare the training data: data from all other partitions\n    partial_train_data = np.concatenate(\n        [train_data[:i * num_val_samples],\n         train_data[(i + 1) * num_val_samples:]],\n        axis=0)\n    partial_train_targets = np.concatenate(\n        [train_targets[:i * num_val_samples],\n         train_targets[(i + 1) * num_val_samples:]],\n        axis=0)\n\n    # Build the Keras model (already compiled)\n    model = build_model()\n    # Train the model (in silent mode, verbose=0)\n    model.fit(partial_train_data, partial_train_targets,\n              epochs=num_epochs, batch_size=1, verbose=0)\n    # Evaluate the model on the validation data\n    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n    all_scores.append(val_mae)\n```\n\n这样获得值如下，2.1~2.6 不等。\n\n```py\nall_scores\n# [2.1905605792999268,\n#  2.4371392726898193,\n#  2.3653202056884766,\n#  2.5255486965179443]\n```\n\n如果想让数据更精确，不如让训练次数从 100 增加到 500，但是书里面最终的数据到 80 就差不多过拟合了，再加上我的笔记本跑不起来 500 次，这里就算了吧。。。\n\n## 预测\n\n```py\nmodel = build_model()\nmodel.fit(train_data, train_targets,\n          epochs=80, batch_size=16, verbose=0)\ntest_mse_score, test_mae_score = model.evaluate(test_data, test_targets)\n```\n\n最后预测的房价和现实房价相差大概 2714 美刀（书里预测的是 2550）.\n","collection":"blog","data":{"type":"post","category":"book","tag":["tensorflow","keras","wsl2","python"],"series":{"slug":"learn-tensorflow","name":"张量麻辣烫"}}},{"id":"2020/06/14/ml5.js-tensorflow.js.md","slug":"2020/06/14/ml5js-tensorflowjs","body":"\n# ml5.js 和 tensorflow.js，终于聊到前端部分了\n\n既然在浏览器中也可以计算多维数组，拿浏览器做深度学习也可以理解了。Google 给浏览器中设计了 [tensorflow.js](https://www.tensorflow.org/js)，跟 python 下面的 tensorflow 是同一套 API。又有一群人在 tensorflow 的基础上封装了一套[ml5.js](https://ml5js.org/)。对比 tensorflow.js，ml5.js 去掉了很多张量计算的部分（说实话，这些东西真不是人学的，我这一周都在研究这些计算...）。所以本文会以 ml5.js 开始。\n\n## ml5.js\n\n这是[Daniel Shiffman](http://www.shiffman.net/)主导的 JS 深度学习库，我特喜欢看他的视频睡觉。这个库的[使用教程](https://learn.ml5js.org/docs/#/reference/index)不能更详细了！\n\n官网的简介，是使用 MobileNet 了`imageClassifier`，这是我的[笔记](https://observablehq.com/@gongbaodd/untitled)，可以用来判断图片、视频中的物体是什么。\n\n另外也可以使用`neuralNetwork`，这是[笔记](https://observablehq.com/@gongbaodd/ml5-js-neural-network)，基本上前面两篇关于 tensorflow 的文章都可以使用它来跑。\n\n使用 ML5.js 很大的简化了 tensorflow 的 API，然而并不是你可以不了解 tensorflow，因为期间会有很多参数难以理解，又不得不回头看它。\n\n## tensorflow.js\n\n基本上会了 python 版本，js 版本就算是个子集了，基本上很多需要的包都有替代。[这里](https://www.tensorflow.org/js/guide/layers_for_keras_users?hl=zh-cn)是一个给 keras 用户使用的 tensorflow.js 指南。另外去强烈建议看看[tensorflow.js 指南](https://www.tensorflow.org/js/guide?hl=zh-cn)。\n\n- `tf.layers` => Keras\n- `@tensorflow/tfjs-vis`原生支持 tensorflow 的数据可视化库（那敢情好啊）\n","collection":"blog","data":{"type":"post","category":"fe","tag":["tensorflow","keras","wsl2","python"],"series":{"slug":"learn-tensorflow","name":"张量麻辣烫"}}}]}